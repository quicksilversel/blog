---
title: 'Kubernetes Secrets Rotation: Versioned Pattern with Instant Rollback'
description: 'How to rotate Kubernetes secrets safely in production using versioned naming, progressive rollout, and instant rollback strategies. No downtime, no incidents.'
topics: ['Kubernetes', 'AWS', 'Secrets Manager']
published: true
date: '2025-07-22'
---

## Intro

We rotated an auth API secret in production. Error rates spiked within 30 seconds. Some pods had the new secret, others kept the old one. The external API had already invalidated the old token. Half our authentication requests failed.

We reverted in 5 minutes, but it exposed a fundamental problem: updating secrets in place doesn't work at scale.

Here's the versioned rotation pattern we built. No downtime. No emergency rollbacks. Just controlled deployments.

## The Problem with In-Place Updates

Traditional approach:

```bash
# Update secret value
kubectl create secret generic auth-api-secrets \
  --from-literal=api_token=<new-value> \
  --dry-run=client -o yaml | kubectl apply -f -

# Restart deployment
kubectl rollout restart deployment/auth-service
```

This fails because:

- **Rolling restarts aren't instant** - Pods restart gradually, creating a mixed state
- **External dependencies invalidate old tokens immediately** - No grace period
- **No version tracking** - Which secret version is running in production?
- **No rollback plan** - Old value is gone, requires scrambling to find backups

In dev, this works. In production with gradual rollouts and external dependencies, it breaks.

## Versioned Secret Pattern

Treat secrets like immutable deployments. Each rotation creates a new versioned secret.

### Naming Convention

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: auth-api-secrets-20250723110000 # YYYYMMDDHHMMSS
data:
  api_token: <base64-encoded-value>
```

Timestamp format gives you:

- **Clear history** - See all versions with `kubectl get secrets | grep auth-api-secrets`
- **Instant rollback** - Keep old secret, revert deployment reference
- **Audit trail** - Match timestamps to deployment history

## Progressive Rollout Strategy

### Step 1: Create New Secret

```bash
kubectl create secret generic auth-api-secrets-20250723110000 \
  --from-literal=api_token=<new-token> \
  -n production
```

Old secret stays. Both exist simultaneously.

### Step 2: Deploy to Staging

```yaml
# staging/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: auth-service
spec:
  template:
    spec:
      containers:
        - name: app
          envFrom:
            - secretRef:
                name: auth-api-secrets-20250723110000
```

Test thoroughly. If it breaks, old secret is still there.

### Step 3: Production Rollout

**Option A: Blue-Green Deployment**

Run both versions temporarily:

```yaml
# Blue (old secret)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: auth-service-blue
spec:
  replicas: 3
  template:
    spec:
      containers:
        - name: app
          envFrom:
            - secretRef:
                name: auth-api-secrets-20250715100000
```

```yaml
# Green (new secret)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: auth-service-green
spec:
  replicas: 3
  template:
    spec:
      containers:
        - name: app
          envFrom:
            - secretRef:
                name: auth-api-secrets-20250723110000
```

Route 10% traffic to green. Monitor. Increase gradually.

**Option B: Rolling Update with Monitoring**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: auth-service
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  template:
    spec:
      containers:
        - name: app
          envFrom:
            - secretRef:
                name: auth-api-secrets-20250723110000
```

Watch error rates. Rollback immediately if spikes occur.

### Step 4: Cleanup (1-2 Weeks Later)

```bash
# Verify no references to old secret
kubectl get deployments --all-namespaces -o yaml | grep "auth-api-secrets-20250715100000"

# Delete old secret
kubectl delete secret auth-api-secrets-20250715100000 -n production
```

Keep old secrets for at least two weeks. Weekly cronjobs might still reference them.

## Instant Rollback

When new secret breaks production:

```bash
# Revert deployment
kubectl rollout undo deployment/auth-service -n production

# Verify old secret exists
kubectl get secret auth-api-secrets-20250715100000 -n production

# Check status
kubectl get pods -n production -l app=auth-service
```

Because old secret still exists, rollback is instant. No scrambling for old token values.

## Environment-Specific Handling

### Development

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: auth-api-secrets # No versioning
data:
  api_token: <dev-token>
```

No versioning needed. Break things and learn.

### Staging

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: auth-api-secrets-20250723110000 # Versioned
data:
  api_token: <staging-token>
```

Full rotation process. Test failures here.

### Production

- Versioned secrets with timestamps
- Progressive rollout
- Monitoring at every step
- Rollback procedures documented
- Old secrets kept 1-2 weeks

## Common Pitfalls

### 1. Forgetting References

**Problem:** Rotated secret but forgot one deployment still referenced old name.

**Solution:** Search before cleanup:

```bash
kubectl get deployments --all-namespaces -o yaml | grep "old-secret-name"
kubectl get pods --all-namespaces -o yaml | grep "old-secret-name"
```

### 2. Skipping Staging

**Problem:** "It's just a token rotation." Updated production directly. Token format changed. Auth broke.

**Solution:** No exceptions. Every secret change goes through staging.

### 3. Deleting Old Secrets Too Soon

**Problem:** Deleted old secret immediately. Weekly cronjob failed three days later.

**Solution:** Keep old secrets for 2+ weeks. Monthly cronjobs need longer.

### 4. No Monitoring

**Problem:** Checked logs manually. Looked fine. Got paged hours later. Errors were sporadic.

**Solution:** Set up monitors for secret rotations:

```yaml
# Datadog monitor (pseudo-config)
monitor:
  name: 'Auth API - High Error Rate'
  query: 'avg(last_5m):sum:api.auth.errors{env:production} > 10'
  message: 'Auth errors spiked. Check recent secret rotation.'
```

Watch error rates for 24 hours after rotation.

## GitOps Integration

Secrets don't belong in Git. Secret _references_ do.

### What Goes in Git

```yaml
# manifests/production/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: auth-service
spec:
  template:
    spec:
      containers:
        - name: app
          envFrom:
            - secretRef:
                name: auth-api-secrets-20250723110000 # Reference only
```

Git contains the name, not the value.

### What Stays Out of Git

Secret creation happens outside GitOps:

```bash
kubectl create secret generic auth-api-secrets-20250723110000 \
  --from-literal=api_token=<value> \
  -n production
```

Or use External Secrets Operator:

```yaml
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: auth-api-secrets-20250723110000
spec:
  secretStoreRef:
    name: aws-secretsmanager
  target:
    name: auth-api-secrets-20250723110000
  data:
    - secretKey: api_token
      remoteRef:
        key: /production/auth-api/token
        version: 20250723110000
```

Operator syncs from AWS Secrets Manager. Value never touches Git.

## Rotation Checklist

**Before Rotation**

- [ ] Create new secret with timestamped name
- [ ] Document rollback procedure
- [ ] Verify monitoring is in place
- [ ] Schedule during low-traffic window

**Staging Deployment**

- [ ] Update staging deployment to new secret
- [ ] Deploy and verify functionality
- [ ] Check logs for auth errors
- [ ] Run integration tests
- [ ] Let run for 24 hours

**Production Deployment**

- [ ] Verify old secret still exists
- [ ] Update production deployment
- [ ] Monitor error rates during rollout
- [ ] Check authentication logs
- [ ] Verify all pods restarted
- [ ] Watch for 24 hours

**Cleanup (1-2 Weeks Later)**

- [ ] Search for all references to old secret
- [ ] Verify no pods use old secret
- [ ] Delete old secret from Kubernetes
- [ ] Delete old secret from external store
- [ ] Update documentation

## Key Takeaways

**Versioning prevents incidents.** Timestamped secret names give instant rollback and clear history.

**Progressive rollout matters.** Staging → Production → Monitor → Cleanup. No shortcuts.

**Keep old secrets longer than you think.** Weekly cronjobs, monthly jobs, obscure scripts all need secrets.

**Automate monitoring.** Can't watch logs manually for every rotation. Set up alerts.

**Secrets aren't config.** They expire. They break unexpectedly. Treat them with extra caution.

## TL;DR

1. Use versioned secret names with timestamps (`secret-name-20250723110000`)
2. Create new secret, keep old one during transition
3. Test in staging first, always
4. Deploy to production gradually with monitoring
5. Keep old secrets for 1-2 weeks minimum
6. Document rollback procedures
7. Use External Secrets Operator for GitOps
8. Set up monitoring for secret rotations

This pattern eliminated our secret rotation incidents. No emergency rollbacks since implementation.
